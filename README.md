# Building_Large_Language_Models_LLM

GPT Version 1: I developed this large language model (LLM) by meticulously training it on The Wizard of Oz and crafting the necessary code components on AWS SageMaker. This project involved building a transformer-based model using PyTorch, optimizing the architecture with multi-head attention and feedforward layers, and leveraging AWS SageMakerâ€™s powerful infrastructure for training.

Despite initial hardware limitations, transitioning to AWS SageMaker ensured efficient training and validation, resulting in a robust language model.
